---
editor_options:
  markdown:
    wrap: 72
---

# Answers {.unnumbered}

*Section 4 Practical: Fitting CFA Models: A Case Study using the SF-36 "Mental Health" scale*


## The Task {-}

Fit a CFA model assuming a single latent factor that is indicated by items "nervous", "down", "peace", "sad", "happy" and include intercepts (`meanstructure`) in the model. Then answer the questions below. 

Let's load the required packages: 

```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(lavaan)
library(psych)
```

Importing the data (in this case, my data file is located in a folder called `data`).

```{r warning = FALSE, message = FALSE}
whitehall <- read_csv("data/whitehall_NA.csv")
```

Let's now specify the model.

```{r}
mh_fac <- 'mh =~ nervous + down + peace + sad + happy'
```

The `=~` operator is read as "is indicated by" or "is measured by". So this translates to: *create a latent factor called 'mh' that is indicated by the five observed variables listed on the right-hand side of the equation*. 

And now we fit the model. 

```{r}
fit_mh_fac <- cfa(mh_fac, data = whitehall, meanstructure = TRUE)
```

Let's view the parameters estimates (standardised and unstandardised) and fit indices 


```{r}
summary(fit_mh_fac, fit.measures = TRUE, standardized = TRUE)
```

And now the residuals.

```{r}
residuals(fit_mh_fac)
```


## Question 1 {-}

**Interpret both the standardised and unstandardised loadings: what do these tell us?**   

The factor loadings are shown in the section starting `mh =~`.  

Two variables (**peace**, **happy**) have negative loadings, the rest positive. This means that the latent factor is "measuring" negative mental health, i.e. people with a high factor score will tend to have high scores on nervous, down, sad and low scores on peace, happy, and vice versa.   

The first loading, for **nervous**, is fixed at $1$ to identify the scale of measurement of the latent variable. (We can tell that it is a fixed, and  not estimated quantity, because it does not have an SE, z-value, p-value, etc.). This means that the latent variable **mh** is measured on the same measurement scale as the observed variable **nervous**, i.e. has the same variance/sd.  

The size of the other loadings are relative to this loading for **nervous**. **Sad** has the highest loading, $1.349$. This means that a one-unit difference in the latent variable would be associated with a $1.349$ unit difference in the score on **sad**.  

The raw, unstandardised units for the loadings can be difficult to interpret because we are not familiar with interpreting the somewhat arbitrary 6-point measurement scale for these items.   

Instead, we often interpret the standardised loadings, which are measured in standard deviations and vary between $-1$ and $1$. These units are found in the `Std.all` column. The standardised loading for **sad** is $0.816$, which means that for a one standard deviation difference in the latent variable, we would expect to see a $0.816$ standard deviation difference in the score of the observed variable **sad**. The variable **nervous** has the weakest loading, perhaps because nervousness does not always reflect poor mental health (e.g. nervous excitement).

## Question 2 {-}

**Interpret the item intercepts: What do these tell us?**  

The item intercepts represent the expected value of the observed variable when the latent variable is zero. Because the latent variable is a standard normal variable, with its mean fixed to zero, the intercepts represent the expected value of the observed variable when the latent variable is at its mean. This is getting a little "chicken and egg", but in effect it just implies that the intercepts are the mean of the observed items.   

The `Estimates` column shows the unstandardised intercepts, i.e. in the original  1-6 scale of the observed variables. Average responses to the positive items (**peace**, **happy**) were much higher than those to the negative items. People reported feeling positive quite frequently, and sad/negative quite rarely.


## Question 3 {-}

**Interpret the residuals: What do these tell us?**  

The residuals are the differences between the observed and predicted values, in this case of the covariances among the observed variables. The residuals were all below $0.1$, except for the covariance between the positive items **happy** and **peace**, which was $0.3$. This suggests a single factor is not picking up the full relationship between these items. This suggests that big proportion of the covariance among these two "positive" items is not being adequately captured by just a single latent factor.

## Question 4 {-}

**Interpret the model fit indices (CFI, SRMR, RMSEA): what do these tell us?**

The model fit indices are shown at the top of the output, starting with `Model Test User Model`. This gives us the chi-square test of model fit and its associated p-value. The chi-square test is a test of the null hypothesis that the model fits the data perfectly. A non-significant p-value (e.g. $p > 0.05$)  would suggest that the model fits the data well. However, the chi-square test is sensitive to sample size, and so is often significant even when the model fits the data well. Our model does not fit the data well by this criterion.  

Next we'll look at the *Comparative Fit Index (CFI)*. This is a measure of how well the model fits the data compared to a null model (a model with zero covariances  among the observed variables). A CFI of $1$ would suggest perfect fit, and a CFI of $0$ would suggest our data fit no better than random data.  The CFI of $0.875$ is fairly poor.  

Next up is the *Root Mean Square Error of Approximation (RMSEA)*. This is a measure  based on the chi-square, but taking into account the complexity of the  model (with complexity represented by estimated model parameters). An RMSEA of $0$ would suggest perfect fit, and a RMSEA of below $0.05$ would suggest  a "close" fit between model and data. Our RMSEA of $0.2$ is considered poor.  

Finally we look at the *Standardized Root Mean Square Residual (SRMR)*. This is a measure of the average of the standardised residuals, i.e. the average error in the model-estimated correlations among the observed variables. An SRMR of $0$ would suggest the model can reproduce the observed correlations  perfectly, with all zero residuals. An SRMR of below $0.08$ is considered good. Our SRMR is $0.06$ is good, but as we see from the residuals this is misleading - most residuals are small but one is very large, so  on average (which is what the SRMR shows), they are small.

## Question 5 {-}

Overall, do you support the claim made by [Ware 2000](https://journals.lww.com/spinejournal/citation/2000/12150/sf_36_health_survey_update.8.aspx) that these five items plausibly reflect a single, latent factor?  

Not good enough. We can see that a single latent construct is not enough to capture the distinction between indicators of negative and positive mental health.

