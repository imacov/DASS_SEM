[["index.html", "SOST70073 Structural Equation Modelling About", " SOST70073 Structural Equation Modelling Dr. Nick Shryane About Welcome to SOST70073 Structural Equation Modelling! This notebook will host the materials for all R practicals and demonstrations for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser. For each section, you will have at least one practical to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually. The structure and options of this R notebook is identical to that of SOST70033 Data Science Modelling. The sidebar menu can be toggled on and off by clicking on the Toggle Sidebar button. Other customisation options include changing the font, font size, and the appearance. You also have a handy search button. To enhance your experience on tablet and mobile devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows. The code, as well as the output and answers are provided for the practicals at the end of each section. The R code can be copied and pasted directly in your R console or script. 1: Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project. 2: You should write and save your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section. 3: The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called data) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R. 4: To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely essential that you first attempt all practical tasks and exercises on your own before comparing your answers with those provided in this notebook. You must also follow along in all demonstrations that contain code on your own machines (simply reading through the text and code will NOT be sufficient to develop a robust understanding of the method and its implementation in R); note that you may also need to reflect on and answer questions throughout a demonstration as well. "],["overview.html", "Overview", " Overview Section 1: What is Structural Equation Modelling? This section is comprised of one demonstration. We are going to run some linear regression models using the familiar lm() library. Then we’re going to run them using functions from the lavaan library. They will be the same models, with (almost) the same results. The aim is to show you that the basic building block of a SEM is just a regression model. Learning Outcomes: use the lavaan package to fit models to data; recognise the similarities and differences between output produce bylm() and lavaan; appreciate the importance of an experimental design and assess hypotheses; build frequency tables using the janitor package; generate dummy variables using the fastDummies package; generate and interpret visualisations. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package rm() remove objects from environment base ls() list objects base getwd() get working directory base setwd() set working directory base read.csv() read .csv file in table format utils tabyl() generate frequency table janitor dummy_cols() create dummy variables fastDummies head() return first part of object utils mutate() create, modify, delete columns dplyr str_replace() replace matches with new text stringr relocate() change column order dplyr arrange() order rows using column names dplyr geom_boxplot() generate box plot ggplot2 geom_line() connect observations ggplot2 geom_point() generate scatterplot ggplot2 facet_wrap() wrap 1-dimensional ribbon of panels into 2 dimensions ggplot2 group_by() group by one or more variables dplyr summarise() summarise each group down to one row dplyr sem() fit structural equation models lavaan summary() generic function for producing result summaries of models base plot() generate plot base var() compute variance stats glimpse() obtain a glimpse of your data dplyr "],["an-introduction-to-lavaan-how-do-wet-fingers-wrinkle.html", "An Introduction to lavaan: How Do Wet Fingers Wrinkle? Workspace Data Experimental Design Frequency for Variables and Levels The Causal Model Statistical Model Preparing the Data Visualing the Data The lavaan Package Tasks and Questions", " An Introduction to lavaan: How Do Wet Fingers Wrinkle? For this demonstration, we will make use of data from the following study: Kareklas, et al. (2013) “Water-induced finger wrinkles improve handling of wet objects”, Biology Letters, http://dx.doi.org/10.1098/rsbl.2012.0999 Workspace First, we’re going to clear the workspace (beware! This will remove everything from the “environment”). rm(list = ls()) Now, let’s set the working directory. You will remember that the working directory is the folder where R will look for and save files to. If you want to know what your current working directory is, run the command below: getwd() To change it, modify the path in the quotation marks \" \" below to your directory. setwd(&quot;C:/Users/...&quot;) Now let’s load the relevant packages. In this demonstration, we’ll be using the lavaan library, but also a few others: fastDummies which will facilitate the generation of dummy variables, and emeans for computing and plotting estimated marginal means. You must install these prior to loading them. install.packages(&quot;lavaan&quot;) install.packages(&quot;fastDummies&quot;) install.packages(&quot;emmeans&quot;) In addition to these, we will also need tidyverse to allow us to make use of the pipe operator, ggplot2 for plotting results and janitor for generating frequency tables. library(&quot;lavaan&quot;) library(&quot;tidyverse&quot;) library(&quot;janitor&quot;) library(&quot;fastDummies&quot;) library(&quot;ggplot2&quot;) library(&quot;emmeans&quot;) Data We’ll download the data from the internet, and store it in a dataframe called wrinkle. wrinkle &lt;- read.csv(&quot;https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Wrinkle.csv&quot;) Experimental Design The skin on our fingers and toes wrinkles when it has been in water for a few minutes. Why on earth does it do this? The skin on almost all other body parts does not wrinkle when wet. Twenty participants were asked to pick up small objects with the right hand thumb and index finger, pass them through a small hole, grab them with the left hand, and put them into a box that had a hole in the lid. For half the trials, participants had their fingers soaked in water, making the skin wrinkle. On the other half of the trials, the fingers were dry. For half the trials the objects were wet. For the other half, the objects were dry. The time it took the participants to finish the task of passing all of the objects into the box was recorded for each condition: Condition 1: Wet/wrinkled fingers, wet objects Condition 2: Wet/wrinkled fingers, dry objects Condition 3: Dry fingers, wet objects Condition 4: Dry fingers, dry objects Hypothesis: Skin wrinkling is an adaptation to improve grip when in wet environments. That’s why we only need it on our fingers (for handling wet objects) and toes (for walking on wet surfaces). Research Question: Will objects be easier/quicker to handle with wrinkled/wet fingers compared to with dry fingers? Frequency for Variables and Levels Fingers were wet/wrinkled half the time wrinkle %&gt;% tabyl(Fingers) ## Fingers n percent ## non 40 0.5 ## wrinkled 40 0.5 Objects were wet half the time wrinkle %&gt;% tabyl(Objects) ## Objects n percent ## dry 40 0.5 ## wet 40 0.5 Participants 1 to 20 (all four conditions) wrinkle %&gt;% tabyl(Participant) ## Participant n percent ## p1 4 0.05 ## p10 4 0.05 ## p11 4 0.05 ## p12 4 0.05 ## p13 4 0.05 ## p14 4 0.05 ## p15 4 0.05 ## p16 4 0.05 ## p17 4 0.05 ## p18 4 0.05 ## p19 4 0.05 ## p2 4 0.05 ## p20 4 0.05 ## p3 4 0.05 ## p4 4 0.05 ## p5 4 0.05 ## p6 4 0.05 ## p7 4 0.05 ## p8 4 0.05 ## p9 4 0.05 The Causal Model This was an experiment. The values of the predictors (Fingers and Objects) were decided by the experimenters, so they are uncorrelated causes of the outcome (Time). Specific Hypotheses: Handling wet objects will take longer than handling dry objects Handling objects with wet fingers will take longer than with dry fingers Handling wet objects will be faster if done with wet/wrinkled fingers than with dry fingers Handling time for dry objects will NOT be affected by whether the fingers are wet or dry Statistical Model The hypothesis is not simply that wet fingers and wet objects will result in slower handling time, it is that wet fingers will reduce the slowing effect of handling wet objects specifically. This implies an interaction between Fingers and Objects. Preparing the Data First we’ll create numerical dummy variables for having wet (wrinkled) fingers and wet objects. wrinkle &lt;- dummy_cols(wrinkle, select_columns = &quot;Fingers&quot;) wrinkle &lt;- dummy_cols(wrinkle, select_columns = &quot;Objects&quot;) We now have two new dummy variables: Fingers_wrinkled and Objects_wet head(wrinkle) ## rownames Participant Time Condition Fingers Objects WrinkledThenNon ## 1 1 p1 106 non-wrinkled/dry non dry 1 ## 2 2 p2 113 non-wrinkled/dry non dry 2 ## 3 3 p3 94 non-wrinkled/dry non dry 1 ## 4 4 p4 96 non-wrinkled/dry non dry 2 ## 5 5 p5 93 non-wrinkled/dry non dry 1 ## 6 6 p6 123 non-wrinkled/dry non dry 2 ## DryThenWet Fingers_non Fingers_wrinkled Objects_dry Objects_wet ## 1 1 1 0 1 0 ## 2 1 1 0 1 0 ## 3 1 1 0 1 0 ## 4 1 1 0 1 0 ## 5 1 1 0 1 0 ## 6 1 1 0 1 0 And then we create an interaction variable for Fingers and Objects. wrinkle$Interaction &lt;- wrinkle$Fingers_wrinkled*wrinkle$Objects_wet Finally, lets’s re-organise the data so that it can be used with ggplot2, so we can visualise the data. We need to convert the Participant variable to a numeric variable so that we can sort it correctly. We’ll use the str_replace() function from the stringr library to remove the “p” from the Participant variable, and then convert it to a numeric variable using as.numeric(). We’ll then use relocate() from dplyr to move the id variable to the second column. Finally, we’ll use arrange() also from dplyr to sort the dataframe by the id variable. We’ll save the sorted dataframe as wrinkle2. wrinkle2 &lt;- wrinkle %&gt;% mutate(id = as.numeric(str_replace(Participant, &quot;p&quot;, &quot;&quot;))) %&gt;% relocate(id, .after = Participant) %&gt;% arrange(id) Visualing the Data Great, now we have a dataset that we can use with ggplot2. We can use either wrinkle or wrinkle2 to analyse the data with lm() and lavaan. Let’s see the completion time for each condition as a boxplot. wrinkle2 %&gt;% ggplot(aes(x = Fingers, y = Time)) + geom_boxplot() + facet_wrap( ~ Objects) We have four boxplots, one for each condition. The thick black horizontal line in the middle of each box is the median time to complete the task. The upper and lower edges of the box are the 75th and 25th percentiles of completion times for each condition. The “whiskers” extend to the most extreme completion times, that represent the 2.5% and 97.5% percentiles of the data. The dots are “outliers” - completion times that are so extreme that they are outside of the whiskers, which represent 95% of the data. It’s clear that handling wet objects with dry (“non” wrinkled) fingers has the highest median handling time. Let’s now take a look at a line graph of the interaction. First, we compute a summary statistic (the mean) for each condition. wrinkle2_summary &lt;- wrinkle2 %&gt;% group_by(Fingers, Objects) %&gt;% summarise(Mean_time = mean(Time)) ## `summarise()` has grouped output by &#39;Fingers&#39;. You can override using the ## `.groups` argument. Then, we plot the above statistics. wrinkle2_summary %&gt;% ggplot(aes(x = Fingers, y = Mean_time, color = Objects)) + geom_line(aes(group = Objects)) + geom_point() The dots show the mean completion time for each condition. The lines connect the dots for the same object type. The lines are not parallel, which suggests that we have an interaction between Fingers and Objects. However, this graph does not show us the uncertainty in the mean completion times. (The graph is also misleading because the y-axis does not begin at zero, which makes the differences across contitions look larger than they are.) To fully assess our hypotheses, we need to fit statistical models of the mean differences, which take into account the uncertainty in the mean completion times, by computing standard errors (SEs) for the differences. The lavaan Package With lavaan, we define the statistical model as an object and we name it model. The object specification is enclosed in single quotes. As you can see below, the model structure is similar to that of lm(). model &lt;- &#39;Time ~ Fingers_wrinkled + Objects_wet + Interaction&#39; To then fit the model to the data, we use the sem() function. fit_lav &lt;- sem(model, data=wrinkle2, meanstructure = TRUE) One difference is that lavaan does not include intercepts in the model by default, so we have to ask for them by including the option meanstructure = TRUE. Let’s take a look at the output. summary(fit_lav) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 80 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Time ~ ## Fingers_wrnkld 0.850 6.257 0.136 0.892 ## Objects_wet 24.600 6.257 3.932 0.000 ## Interaction -15.900 8.849 -1.797 0.072 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Time 93.300 4.424 21.088 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Time 391.489 61.900 6.325 0.000 As you can see, there is a lot more output than from lm()! For now, we will focus on the part starting at regressions. Tasks and Questions Task 1 Fit a linear model using lm() to evaluate the effect of the predictors Fingers and Objects, and their interaction, on outcome Time. Are the specific hypotheses supported by the data? Task 2 What are the differences between the lm() and lavaan results? Why might these differences exist? Task 3 How have the results changed after relaxing the independence assumption? Would your answer to the research question change? "],["answers.html", "Answers Task 1 Task 2 Task 3", " Answers Section 1 Demonstration: An Introduction to lavaan: How Do Wet Fingers Wrinkle? Task 1 Fit a linear model using lm() to evaluate the effect of the predictors Fingers and Objects, and their interaction, on outcome Time. Are the specific hypotheses supported by the data? Let’s fit the model. fit_lm &lt;- lm(Time ~ Fingers_wrinkled + Objects_wet + Interaction, data = wrinkle2) summary(fit_lm) ## ## Call: ## lm(formula = Time ~ Fingers_wrinkled + Objects_wet + Interaction, ## data = wrinkle2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.900 -15.113 -2.025 11.925 80.100 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 93.300 4.539 20.554 &lt; 2e-16 *** ## Fingers_wrinkled 0.850 6.420 0.132 0.89501 ## Objects_wet 24.600 6.420 3.832 0.00026 *** ## Interaction -15.900 9.079 -1.751 0.08391 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.3 on 76 degrees of freedom ## Multiple R-squared: 0.1997, Adjusted R-squared: 0.1681 ## F-statistic: 6.323 on 3 and 76 DF, p-value: 0.0006927 If we explore the residuals, we can note that none of the plots reveals a major problem. plot(fit_lm) ## hat values (leverages) are all = 0.05 ## and there are no factor predictors; no plot no. 5 Substantive interpretation of the coefficients: Coefficient Value Interpretation Intercept 93.300 Mean time to handle dry objects with dry hands Fingers_wrinkled 0.850 Additional time to handle dry objects with wet hands Objects_wet 24.600 Additional time to handle wet objects with dry hands Interaction Interaction 15.900 Difference from above when handling wet objects with wet hands The pattern of coefficients supports the hypotheses: Handling wet objects with dry hands takes on average \\(24.6\\) seconds longer than handling dry objects with dry hands, but if the hands are wet when handling the wet objects, it takes \\(15.9\\) seconds less than this: It takes \\(24.6\\) seconds longer to handle wet objects with dry hands than dry objects with dry hands. It takes \\(24.6 - 15.9 = 8.7\\) seconds longer to handle wet objects with wet hands than it takes to handle dry objects with dry hands. However, the SE for the Interaction is \\(9.079\\), so the \\(95%\\) confidence interval is: Lower Bound \\(CI_{95\\%} = -15.9 - (2 \\times 9.079) = -34.058\\) Upper bound \\(CI_{95\\%} = -15.9 + (2 \\times 9.079) = 2.258\\) This shows us the plausible range of values for the interaction in the population. This range includes zero - the effect could plausibly be zero in the population. The interaction effect is therefore non-significant. The key hypothesis of an interaction between Fingers and Wrinkled is not supported by the results of this model. Task 2 What are the differences between the lm() and lavaan results? Why might these differences exist? The estimates for the b coefficients are exactly the same. The standard errors are all a bit smaller. This is because this model was not estimated using OLS. By default, lavaan uses “maximum likelihood” (ML) estimation. ML uses an iterative algorithm that selects the model parameters that were “most likely” to have resulted in the observed data. ML makes an additional assumption that OLS does not - it assumes the residuals are normally distributed. If the residuals are plausibly from a normal distribution, the ML estimator is more “efficient”, especially in small samples. “Efficient” means that it has smaller standard errors. Because the SEs are slightly smaller, the p-values are also slightly lower. However, the Interaction effect is still non-significant (\\(p=0.072\\)). lavaan hasn’t given us \\(R^2\\), but we can work it out for ourselves: \\(R^2 = \\frac{(var_0 - var_1)}{var_0}\\), where \\(var_0\\) is the variance of the outcome variable, and \\(var1\\) is the variance of the residuals after fitting the model. From the output, we find that \\(var_1\\) (residual variance of Time) is \\(391.489\\). We can then obtain \\(var_0\\) by: var_0 &lt;- var(wrinkle2$Time) Therefore, \\(R^2 = \\frac{(495.3899 - 391.489)}{495.3899} = 0.21\\) This is slightly higher than the value computed by lm(), because lavaan uses the sample size \\(N\\) to compute variances, rather than \\(N-1\\) as lm() does. TIP: To get the same \\(R^2\\) as lm(), include the option: likelihood = \"wishart\" in the model estimation command. An assumption of both lm() and lavaan models above is that the observations are independent. Let’s see: glimpse(wrinkle2) ## Rows: 80 ## Columns: 14 ## $ rownames &lt;int&gt; 1, 21, 41, 61, 2, 22, 42, 62, 3, 23, 43, 63, 4, 24, 4… ## $ Participant &lt;chr&gt; &quot;p1&quot;, &quot;p1&quot;, &quot;p1&quot;, &quot;p1&quot;, &quot;p2&quot;, &quot;p2&quot;, &quot;p2&quot;, &quot;p2&quot;, &quot;p3&quot;,… ## $ id &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,… ## $ Time &lt;int&gt; 106, 139, 107, 118, 113, 138, 97, 106, 94, 136, 117, … ## $ Condition &lt;chr&gt; &quot;non-wrinkled/dry&quot;, &quot;non-wrinkled/wet&quot;, &quot;wrinkled/dry… ## $ Fingers &lt;chr&gt; &quot;non&quot;, &quot;non&quot;, &quot;wrinkled&quot;, &quot;wrinkled&quot;, &quot;non&quot;, &quot;non&quot;, &quot;… ## $ Objects &lt;chr&gt; &quot;dry&quot;, &quot;wet&quot;, &quot;dry&quot;, &quot;wet&quot;, &quot;dry&quot;, &quot;wet&quot;, &quot;dry&quot;, &quot;wet… ## $ WrinkledThenNon &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1,… ## $ DryThenWet &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Fingers_non &lt;int&gt; 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,… ## $ Fingers_wrinkled &lt;int&gt; 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,… ## $ Objects_dry &lt;int&gt; 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,… ## $ Objects_wet &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,… ## $ Interaction &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,… There are only 20 participants, but there are 80 rows of data: Each participant did the task four times, and has four rows of data. These observations are therefore not independent, they are clustered within participants. This violates the independence assumption of the residuals in these simple linear models. What can we do? With lavaan, we can relax this assumption by allowing for the clustering when computing the model SEs (the independence assumption only affects the SEs, not the b-values themselves. fit_lav_clus &lt;- sem(model, data = wrinkle2, meanstructure = TRUE, cluster = &quot;Participant&quot;) The cluster command tells lavaan that the observations (rows) are clustered within values of Participant. It then computes “robust” SEs, taking this non-independence into account. summary(fit_lav_clus) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 80 ## Number of clusters [Participant] 20 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 0.000 0.000 ## Degrees of freedom 0 0 ## ## Parameter Estimates: ## ## Standard errors Robust.cluster ## Information Observed ## Observed information based on Hessian ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Time ~ ## Fingers_wrnkld 0.850 2.573 0.330 0.741 ## Objects_wet 24.600 3.833 6.418 0.000 ## Interaction -15.900 2.365 -6.722 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Time 93.300 3.191 29.243 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Time 391.489 139.008 2.816 0.005 Task 3 How have the results changed after relaxing the independence assumption? Would your answer to the research question change? YES! The values of the b-coefficients have not changed, but the SEs are now corrected for clustering within participants and are now very much smaller than before. (This is unusual - for much social data we would expect the SEs to be larger after correction for clustering, but this was a well-designed experiment where the clustering was “explained” by the design.) The SE for the Interaction is \\(2.365\\), so the \\(95%\\) confidence interval is: Lower Bound \\(CI_{95\\%} = -15.9 - (2 \\times 2.365) = -20.63\\) Upper bound \\(CI_{95\\%} = -15.9 + (2 \\times 2.365) = -11.17\\) The \\(95%\\) confidence interval does NOT include zero! The plausible population values for the improvement in handling time for wet objects when the hands are wet/wrinkled compared to dry is now between \\(11.17\\) and \\(20.63\\) seconds faster. In summary, lavaan can do everything that lm() can do, and much, much more as we will see. But be aware that there are differences, for example in the default methods for model estimation (ML vs OLS). "],["overview-1.html", "Overview", " Overview Section 2: Paths and Mediation This section is comprised of one demonstration. We will use simulated data to explore cross-sectional mediation models using lavaan. For detailed information on the lavaan package, please see: https://lavaan.ugent.be/tutorial/ Learning Outcomes: use the lavaan library to simulate data from a mediation model; fit a mediation model; compute mediation (direct and indirect) effects, and interpret those effects; appreciate the importance of an experimental design and assess hypotheses; use the lavaanPlot library to plot a path diagram of the model; understand the effects of unmodelled confounders on estimates. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package set.seed() random number generation base simulateData() simulate data from a Lavaan model syntax lavaan head() return first part of object utils round() round numbers base cor() compute correlation stats cov() compute covariance stats sem() fit structural equation models lavaan lavaanPlot() plot lavaan path model lavaanPlot summary() generic function for producing result summaries of models dplyr plot() plot data objects base density() kernel density estimation stats "],["fitting-and-interpreting-a-cross-sectional-mediation-model.html", "Fitting and interpreting a cross-sectional mediation model 1. Creating a Simulated Dataset 2. Estimating the Mediation Model 3. Model results Tasks and Questions", " Fitting and interpreting a cross-sectional mediation model Before we start, install the lavaanPlot package. install.packages(&#39;lavaanPlot&#39;) And then load the installed package as well as the lavaan package. library(lavaan) library(lavaanPlot) 1. Creating a Simulated Dataset We will create a simulated dataset called data_Med which will contain 500 cases (the default) of three variables: x, the exogenous cause (“exogenous” means it is not caused by any other variable in the model); m, the mediator (which is a outcome of x, but an “endogenous” cause of y); y, the outcome. The Population Model The 500 cases we will create will be drawn randomly from a “population model”. This is the model of the causal relationships that exist in the population between the variables. We will then “draw” a sample of 500 cases from this population, each with a value of x, m, and y, that is a result of the model. Here is an example of a population model: \\[ y \\sim 0.5m + 0.2x + e \\] This means that variable \\(y\\) is a function of variables \\(m\\) and \\(x\\). The \\(b\\) coefficient in the population for m predicting y is \\(0.5\\), in standardised units (i.e. for one standard deviation difference in m we would expect to see one half of a standard deviation difference in y). The \\(b\\) coefficient for x predicting y is \\(0.2\\), so for one standard deviation in x we would expect to see \\(0.2\\) standard deviations difference in y. The value of y is also affected by \\(e\\). This is the error term, which is just a random value that is added to y, to represent the fact that m and x are not the only causes of y - there will be other causes that we have not included in the model. The mean of \\(e\\) is \\(0\\), so that on average it doesn’t bias the values of y systematically up or down. Its standard deviation is \\(1\\). Here is the population model for our mediation model. It is more complex than the example above, because there are two endogenous variables (i.e. variables caused by something else in the model). We don’t need to specify the error terms, because they are included by default, but both m and y will have error terms. \\[ \\begin{aligned} \\text{pop_Med} \\quad &amp; \\begin{cases} m &amp; \\sim -0.6x \\\\ y &amp; \\sim 0.9m + 0.1x \\end{cases} \\end{aligned} \\] What is this a model of? It’s designed to represent the example of dental health in the material for mediation models. y is the outcome, dental health, e.g. level of tooth decay. x is tooth brushing, which is a causal influence on y. m is a measure of bacteria in the mouth. This is a cause of tooth decay, but it itself influenced (negatively) by tooth brushing. For the purposes of this simulation, we will assume that variables x, m, and y, have been standardised (i.e. they have a mean of \\(0\\) and a standard deviation of \\(1\\)). The model shows that tooth decay (y) is strongly positively influenced by bacteria (m), and weakly influenced by tooth brushing (x). However, tooth brushing has a pretty strong negative affect on bacteria (\\(-0.6\\)). Let’s now generate 500 cases based on this population model. First we set a “seed” number so we can re-create exactly the same data every time. set.seed(1234) Then we generate the data. pop_Med &lt;- &#39; m ~ -0.6*x y ~ 0.9*m + 0.1*x &#39; data_Med &lt;- simulateData(pop_Med) Now, let’s take a look at the first few rows of the data head(data_Med) ## m y x ## 1 0.52600181 2.36709958 -0.2363440 ## 2 0.06079546 -1.02468167 -0.7180904 ## 3 -1.99379425 -0.66757120 0.7562848 ## 4 2.78590294 2.84225973 -1.2425802 ## 5 -0.41943615 0.09772246 1.8211988 ## 6 -1.39856627 -0.26645819 -0.4124773 The values of x were not specified in the population model, as they are exogenous. For the model, they just represent draws from a standard normal distribution.We can see this distribution by plotting the density of x. plot(density(data_Med$x)) Let’s have a look at the sample correlations. round(cor(data_Med), 3) ## m y x ## m 1.000 0.733 -0.610 ## y 0.733 1.000 -0.416 ## x -0.610 -0.416 1.000 The correlation between x and m is \\(-0.61\\), which is very close to the value of \\(0.6\\) in the population model, but it not exactly the same because we have random error in our model. The interpretation of the correlations between y and the other variables is not so straightforward, because they are affected by more than one variable. When we analyse these data with lavaan, we will actually be analysing the covariances among the variables, not the correlations. The covariances are unstandardised, and so are dependent by the scale of the variables. They are not bound to be between \\(-1\\) and \\(1\\), as correlations are. round(cov(data_Med), 3) ## m y x ## m 1.363 1.227 -0.730 ## y 1.227 2.058 -0.612 ## x -0.730 -0.612 1.050 2. Estimating the Mediation Model We now have a dataset to analyse called data_Med. We next need to specify a statistical model that we will use to analyse the data. Note that we have a big advantage here compared to poor researchers who analyse messy, real world data: we know the true model that generated the data. 2.1 Specify the statistical model: model_Med &lt;- &#39; m ~ x y ~ m + x &#39; You’ll see that this looks just like the population model! The only difference is that we haven’t specified the strengths of relationships among the variables. These will be estimated from the data. Now we fit the model to the data. fit_Med &lt;- sem(model_Med, data = data_Med) We can use the lavaanPlot library to plot the path diagram implied by the analysis model. lavaanPlot(model = fit_Med) From the plot we see that: The indirect path is: \\(x \\rightarrow m\\) (path \\(a\\)) and \\(m \\rightarrow y\\) (path \\(b\\)) The full indirect path is \\(a*b\\). The direct effect is:  \\(x \\rightarrow y\\) (path \\(c&#39;\\)) We can look at the table of results. summary(fit_Med) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## m ~ ## x -0.695 0.040 -17.214 0.000 ## y ~ ## m 0.938 0.047 19.907 0.000 ## x 0.069 0.054 1.281 0.200 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .m 0.854 0.054 15.811 0.000 ## .y 0.947 0.060 15.811 0.000 Like the results from the simple regression model, the \\(b\\) coefficients telling us the strengths of the relationships among the variables are in the section under Regressions. The results aren’t easy to interpret unless we remember which regressions belong to which paths. For example, which regression represents the \\(c&#39;\\) mediation path? (It’s y ~ x, which is \\(0.069\\), but that’s not obvious). We can make the results easier to interpret by adding labels to the coefficients. We can also use these labels to get lavaan to compute the indirect and total effects for us. 2.2 Adding labels We are going to add a text label to each arrow/path in the diagram: The labels are \\(a\\), \\(b\\), and \\(c\\) \\(a\\) is for the path between predictor x and mediator m variable \\(b\\) is for the path between the mediator m and the outcome y variable \\(c\\) is for the path between the predictor x and the outcome y variable We write each label before the variable name, and we “attach” the label by multiplying it with the variable: model_Med_lab &lt;- &#39; m ~ a*x y ~ b*m + c*x &#39; Note that I’ve used the label \\(c\\) rather than \\(c&#39;\\), because the prime symbol \\(&#39;\\) is used by lavaan to encapsulate the model specification. Now we fit the model again: fit_Med_lab &lt;- sem(model_Med_lab, data = data_Med) And explore the revised results tables summary(fit_Med_lab) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## m ~ ## x (a) -0.695 0.040 -17.214 0.000 ## y ~ ## m (b) 0.938 0.047 19.907 0.000 ## x (c) 0.069 0.054 1.281 0.200 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .m 0.854 0.054 15.811 0.000 ## .y 0.947 0.060 15.811 0.000 The coefficients are now labelled to show which mediation path they represent. 2.3 Computing the indirect and total effects The key effects in mediation are: The direct effect: the size of \\(x \\rightarrow y\\) that is NOT mediated by m The indirect effect: the size of \\(x \\rightarrow y\\) that IS mediated by m The total effect: The sum of the two paths above. The size of the direct effect is given in the standard model output - it’s just the \\(c\\) path. The indirect and total effects are compound effects - they are composed of the combination of more than one individual coefficient. The indirect effect is the product of paths a and b: \\(indirect = a*b\\) The total effect is the indirect plus direct effects: \\(total = (a*b) + c\\) We can get lavaan to compute the sizes and standard errors of these effects by using the := operator. This creates a new coefficient in the output that is defined and computed based on combinations of other labelled coefficients in the model. The syntax is thus: New_coefficient := functions_of_existing_coefficients. e.g. indirect_effect := a*b. Therefore: ab := a*b computes the indirect effect \\((a1*b2)\\) and tot := (a*b) + c computes total effect model_Med_lab &lt;- &#39; m ~ a*x y ~ b*m + c*x ab := a*b tot := (a*b) + c &#39; Now we fit the model again. fit_Med_lab &lt;- sem(model_Med_lab, data = data_Med) And ask for the results. summary(fit_Med_lab) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## m ~ ## x (a) -0.695 0.040 -17.214 0.000 ## y ~ ## m (b) 0.938 0.047 19.907 0.000 ## x (c) 0.069 0.054 1.281 0.200 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .m 0.854 0.054 15.811 0.000 ## .y 0.947 0.060 15.811 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab -0.651 0.050 -13.021 0.000 ## tot -0.583 0.057 -10.239 0.000 3. Model results We’ll plot the path diagram again, this time showing the coefficients. lavaanPlot(model = fit_Med, coefs = T) This is useful, but it doesn’t show the SEs, and it doesn’t show the new derived effects (total, indirect). Here are the results showing the direct (c), indirect (ab), and total (tot) effects: Regressions \\[ \\begin{array}{cccccc} \\text{ } &amp; \\text{Estimate} &amp; \\text{Std.Err} &amp; \\text{z-value} &amp; \\text{P(&gt;|z|)} \\\\ \\hline x (c) &amp; 0.069 &amp; 0.054 &amp; 1.281 &amp; 0.200 \\\\ \\end{array} \\] Defined Parameters \\[ \\begin{array}{cccccc} \\text{} &amp; \\text{Estimate} &amp; \\text{Std.Err} &amp; \\text{z-value} &amp; \\text{P(&gt;|z|)} \\\\ \\hline ab &amp; -0.651 &amp; 0.050 &amp; -13.021 &amp; 0.000 \\\\ tot &amp; -0.583 &amp; 0.057 &amp; -10.239 &amp; 0.000 \\\\ \\end{array} \\] What do these results tell us? For every unit increase in tooth brushing (x), we would expect to see a \\(0.651\\) unit decrease in tooth decay, attributable to the effect of tooth brushing on the reduction of bacteria. (We know it’s because of a reduction in bacteria because the “\\(a\\)” path is negative and the “\\(b\\)” path is positive, i.e. tooth brushing reduces bacteria, but bacteria increase tooth decay.) The direct effect of tooth brushing on tooth decay (“\\(c\\)”) is \\(0.069\\). This is the effect of tooth brushing on tooth decay that is NOT mediated by bacteria. It is positive because tooth brushing actually harms the teeth a little bit! However, the size of this \\(c\\) path is small and actually not statistically significant. (Note, however, that we KNOW this direct effect is not zero in the population, because we designed the population model to have a direct effect of \\(0.1\\). The effect is non-significant here because of sampling noise and random error. The effect of these random influences would be less if we had chosen a larger sample size than \\(N = 500\\).) Tasks and Questions What is the model predicted average difference in the outcome variable y between two people with 1 SD difference in the predictor x? What proportion of the difference in Q1 above is due to the effect of the mediator m? So far, we have assumed that all relevant causes have been included in the model. Instead, what if there is a variable, z that is a cause of x, m, and y, but that is not included in the model? i.e. a confounder. 3.1. Create a population model, identical to pop_Med above, but which additionally contains the influence of a variable z that is a predictor of x, m, and y. The size of the influence of z on these other variables should be \\(0.1\\) in the population model. 3.2. Simulate a new dataset, based upon your new population model. Evaluate the covariances among x, m, y, and z. How do the covariances among x, m, and y compare to those in the original data (the data without the confounder z)? 3.3. Fit the model model_Med to the new data. This model does not contain z and is therefore mis-specified, i.e. it does not include the effect of the variable z. Compare the a, b, c’ paths in the original and new models. How do they differ? Compare the total, direct, and indirect paths in the original and new models. How do they differ? In particular, would you change your inference about the importance of the the mediator? "],["answers-1.html", "Answers Question 1 Question 2 Question 3", " Answers Section 2 Demonstration: Fitting and interpreting a cross-sectional mediation model Question 1 What is the model predicted average difference in the outcome variable y between two people with 1 SD difference in the predictor x? This question asks about the TOTAL effect of x on y. This is \\((a*b) + c\\). All variables are standard normal, so the units of x are SDs already. The size of \\(a*b = -0.651\\) The size of \\(c = 0.069\\) Total effect \\(a*b + c = -0.582\\), which is actually slightly different to the total effect shown of \\(-0.583\\) because of rounding error. For a one unit/SD difference in x, we expect a \\(-0.582\\) unit difference in y. Basically, if you brush your teeth more, you will have a lot less tooth decay. Question 2 What proportion of the difference in Q1 above is due to the effect of the mediator m? The indirect effect \\(a*b\\) is the only path that involves m. Its size is \\(-0.651\\), which is \\(\\frac{-0.651}{-0.583} = 1.117\\), or about \\(111.7\\%\\), of the total effect. What is going on!? The indirect effect is larger than the total effect! This is because the two effects of x on y (direct and indirect) have opposite signs, so when we consider them together, they cancel out a little bit. Think about that for a while…To recover the total effect of x on y, we had to include the effect of m. If we were to just look at the bivariate relationship between x and y, e.g. by just looking at their correlation, we would underestimate the size of their relationship (Incidentally, this why it is a flawed strategy to pick covariates in a regression model based upon their correlation with the outcome variable.) Question 3 So far, we have assumed that all relevant causes have been included in the model. Instead, what if there is a variable, z that is a cause of x, m, and y, but that is not included in the model? i.e. a confounder. Question 3.1 Create a population model, identical to pop_Med above, but which additionally contains the influence of a variable z that is a predictor of x, m, and y. The size of the influence of z on these other variables should be \\(0.1\\) in the population model. We will produce a new population dataset that includes the effect of a confounding variable, z, which is a predictor of x, m, and z. The strength of the effect of z on all other variables is \\(0.1\\) SDs. pop_Med_conf &lt;- &#39; x ~ 0.1*z m ~ -0.6*x + 0.1*z y ~ 0.9*m + 0.1*x + 0.1*z &#39; Question 3.2 Simulate a new dataset, based upon your new population model. Evaluate the covariances among x, m, y, and z. How do the covariances among x, m, and y compare to those in the original data (the data without the confounder z)? set.seed(12345) data_Med_conf &lt;- simulateData(pop_Med_conf) round(cor(data_Med_conf), 3) ## x m y z ## x 1.000 -0.466 -0.246 0.133 ## m -0.466 1.000 0.699 0.008 ## y -0.246 0.699 1.000 0.073 ## z 0.133 0.008 0.073 1.000 The x, z correlation is a bit over-target at 0.13, but that’s OK. Question 3.3 Fit the model model_Med_lab to the new data. This model does not contain z and is therefore mis-specified, i.e. it does not include the effect of the variable z. Compare the \\(a\\), \\(b\\), \\(c&#39;\\) paths in the original and new models. How do they differ? Compare the total, direct, and indirect paths in the original and new models. How do they differ? In particular, would you change your inference about the importance of the the mediator? The data has changed but we are fitting the same model as before, so we can use the same model specification (model_Med_lab). model_Med_lab &lt;- &#39; m ~ a*x y ~ b*m + c*x ab := a*b tot := (a*b) + c &#39; Then we fit the model. fit_Med_noconf &lt;- sem(model_Med_lab, data = data_Med_conf) #, se = &quot;bootstrap&quot;) summary(fit_Med_noconf) ## lavaan 0.6-18 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## m ~ ## x (a) -0.553 0.047 -11.768 0.000 ## y ~ ## m (b) 0.917 0.044 20.825 0.000 ## x (c) 0.149 0.052 2.851 0.004 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .m 1.061 0.067 15.811 0.000 ## .y 1.030 0.065 15.811 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab -0.507 0.050 -10.246 0.000 ## tot -0.358 0.063 -5.663 0.000 As before, we are interested in the information presented under the Regressions and Defined Parameters headings. Regressions \\[ \\begin{array}{cccccc} \\text{ } &amp; \\text{Estimate} &amp; \\text{Std.Err} &amp; \\text{z-value} &amp; \\text{P(&gt;|z|)} \\\\ \\hline x (c) &amp; 0.149 &amp; 0.052 &amp; 2.851 &amp; 0.004 \\\\ \\end{array} \\] Defined Parameters \\[ \\begin{array}{cccccc} \\text{} &amp; \\text{Estimate} &amp; \\text{Std.Err} &amp; \\text{z-value} &amp; \\text{P(&gt;|z|)} \\\\ \\hline ab &amp; -0.507 &amp; 0.050 &amp; -10.246 &amp; 0.000 \\\\ tot &amp; -0.358 &amp; 0.063 &amp; -5.663 &amp; 0.000 \\\\ \\end{array} \\] Because the confounder z was a positive predictor, it has increased the size of the direct effect, which is also positive, and has decreased the sizes of the indirect and total effects, which are negative. The size of the direct path \\(c\\) has been overestimated so much that it is now statistically significant. The size of the total and indirect effects are now a lot lower than they should be. In this mis-specified model, we would seriously underestimate the benefits of brushing teeth, and seriously over-estimate the risks. This is why it is so important to include all relevant variables in a model. (Note, however, that the key word here is “relevant”. Including an unnecessary variable in your model can also be a problem, because it can potentially introduce bias.) "],["overview-2.html", "Overview", " Overview Section 3: Confirmatory Factor Analysis This section is comprised of one demonstration in which we will use Confirmatory Factor Analysis (CFA) to understand measurement scales using lavaan. For a thorough discussion of how CFA models are specified in lavaan, see: https://www.lavaan.ugent.be/tutorial/cfa.html In life, and also in CFA, measurement units are important. Because CFA is often applied to data with somewhat unfamiliar and seemingly arbitrary measurement units, we can forget that the parameters from CFA are uniquely defined by the units of measurement of the data we use. We will investigate what I mean by this using simulated data on people’s height: What if we have three fallible measures of a person’s height? What if these measurements have been made using different measurement units? Learning Outcomes: simulate data; summarise and visualise data; build, fit, and interpret the results of a CFA model. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package set.seed() random number generator base data.frame() create data frame base rnorm() generate normal distribution stats mutate() create, modify, delete columns dplyr plot() plot data objects base density() kernel density estimation stats sapply() apply function over list or vector base mean(), median(), sd() compute mean, median, standard deviation respectively base print() print values base cor() compute correlation stats cfa() fit CFA models lavaan summary() generic function for producing result summaries of models base "],["confirmatory-factor-analysis-simulating-height.html", "Confirmatory Factor Analysis: Simulating Height Generating the Data CFA Models of the Data BONUS", " Confirmatory Factor Analysis: Simulating Height Let’s load the relevant packages first. library(lavaan) library(tidyverse) In this demonstration, we will simulate three observed measurements: one measure taken in meters, one in feet, and one in inches. We are then going to analyse these measures with a CFA model, and see how it deals with the different measurement scales. For those of you not familiar with feet and inches, they are archaic measurements of length that, although they have been superseded by a more modern measurement unit - the metre - are still in common use in the UK for certain things, notably height. They relate to metres as follows: 1 metre = 3.28 feet (1 foot = 1/3.28 = 0.305 metres) 1 metre = 39.37 inches (1 inch = 1/39.37 = 0.025 metres) (There are 12 inches in one foot, i.e. 39.37/3.28 = 12) In this example, these different measures will be equally “good” measures of height, i.e. they will all contain the same proportion of measurement error. The only major difference is that they will be in these different units. This example will show us how the CFA model effortlessly equates the units of different observed variables, and relates them all to the same underlying scale that is used by the latent variable. Generating the Data We want to create 3 observed measurements of a person’s height: y1 will represent true height (F) plus measurement error (e), in metres y2 will represent true height (F) plus measurement error (e), in feet y3 will represent true height (F) plus measurement error (e), in inches These measurements will be generated using the CFA equation. We will start by creating a variable that reflects a persons “true” height (i.e. the latent, unobserved variable F). Then we will create variables to reflect the measurement error for the observed measures. We will create observed height measures as follows: observed_height_1 = true_height + measurement error_1 observed_height_2 = true_height + measurement error_2 observed_height_3 = true_height + measurement error_3 The equations above are nothing but a simple CFA model, one which does not have any item intercepts or factor loadings. An equivalent way to think of it is that the intercepts and loadings are the same for measurements 1 to 3. The additional thing we will do is we will re-scale the measures so that 2 and 3 reflect measurements taken in feet and inches, respectively. When we have simulated these three observed, error prone measures of height in the three units, we will analyse the data with a CFA model and watch how, as if by magic, the model uses the loading and intercept parameters of the model to recover the measurement units of the three measurement units. Height will be simulated based on European data from: https://en.wikipedia.org/wiki/Average_human_height_by_country Where: f = true height: mean 1.7 m, sd = 0.10 m e = measurement error: mean 0, sd = 0.02 m This means that the observed measures of height will vary from the true height by 2 cm on average. We will generate three observed measures of height (y1, y2, y3), each with it’s own measurement error. The sample size will be \\(N = 500\\). First, we create a dataframe with the true height (f) and the measurement errors e1, e2, and e3. set.seed(1234) height_data &lt;- data.frame(f = rnorm(500, 1.7, 0.1 ), e1 = rnorm(500, 0, 0.02), e2 = rnorm(500, 0, 0.02), e3 = rnorm(500, 0, 0.02) ) The rnorm(n, mean, sd) function will generate \\(n\\) observations, each drawn randomly from a normal distribution with the specified mean and standard deviation. Let’s now create the observed variables. y1 is already in metres, so we don’t need to re-scale it, y2 needs to be re-scaled to represent feet (i.e. we must multiply its values by 3.28), and y3 needs to be re-scaled to represent inches (i.e. we must multiply its values by 39.37) height_data &lt;- height_data %&gt;% mutate(y1_metres = f + e1, y2_feet = (f + e2)*3.28, y3_inches = (f + e3)*39.37 ) Let’s have a look at the data plot(density(height_data$y1_metres)) plot(density(height_data$y2_feet)) plot(density(height_data$y3_inches)) Note that the distributions are on the different scales (metres, feet, inches), and are slightly different shapes because of measurement error. For a summary of the descriptive statistics: desc_stats &lt;- data.frame( Mean = sapply(height_data, mean), Median = sapply(height_data, median), SD = sapply(height_data, sd) ) print(desc_stats) ## Mean Median SD ## f 1.7001838821 1.697929266 0.10348139 ## e1 -0.0011006645 -0.001267044 0.01917183 ## e2 0.0006335859 0.001332594 0.01875223 ## e3 -0.0000532562 -0.001031528 0.02047146 ## y1_metres 1.6990832176 1.697436416 0.10677492 ## y2_feet 5.5786812949 5.571993572 0.34980698 ## y3_inches 66.9341427404 66.808852838 4.14919324 And correlations: cor(height_data) ## f e1 e2 e3 y1_metres ## f 1.000000000 0.08188932 0.08086834 -0.004835441 0.983858046 ## e1 0.081889315 1.00000000 -0.02650801 0.032107889 0.258917123 ## e2 0.080868336 -0.02650801 1.00000000 -0.009471980 0.073614301 ## e3 -0.004835441 0.03210789 -0.00947198 1.000000000 0.001078801 ## y1_metres 0.983858046 0.25891712 0.07361430 0.001078801 1.000000000 ## y2_feet 0.984522857 0.07479654 0.25429895 -0.006357324 0.967584768 ## y3_inches 0.980953427 0.08664333 0.07756414 0.189497465 0.966252575 ## y2_feet y3_inches ## f 0.984522857 0.98095343 ## e1 0.074796539 0.08664333 ## e2 0.254298951 0.07756414 ## e3 -0.006357324 0.18949746 ## y1_metres 0.967584768 0.96625257 ## y2_feet 1.000000000 0.96546091 ## y3_inches 0.965460915 1.00000000 The observed height measures (y1_metres, y2_feet, y3_inches) are very, very highly correlated, as we would expect. They are basically the same measure, with just a bit of measurement error added in. CFA Models of the Data Let’s now specify our CFA model using the three observed measures (metres, feet, inches) of true height. height_metres &lt;- &#39; Height =~ y1_metres + y2_feet + y3_inches &#39; We read the above model as follows: the latent variable Height is indicated by =~ three observed variables: y1_metres, y2_feet, and y3_inches. Let’s fit the model to the data. fit_height_metres &lt;- cfa(height_metres, data = height_data) And have a look at the results. summary(fit_height_metres, standardized = TRUE, rsquare = T) ## lavaan 0.6-18 ended normally after 42 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Height =~ ## y1_metres 1.000 0.105 0.984 ## y2_feet 3.273 0.039 84.821 0.000 0.344 0.983 ## y3_inches 38.774 0.466 83.185 0.000 4.070 0.982 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1_metres 0.000 0.000 9.632 0.000 0.000 0.032 ## .y2_feet 0.004 0.000 9.937 0.000 0.004 0.033 ## .y3_inches 0.616 0.059 10.413 0.000 0.616 0.036 ## Height 0.011 0.001 15.306 0.000 1.000 1.000 ## ## R-Square: ## Estimate ## y1_metres 0.968 ## y2_feet 0.967 ## y3_inches 0.964 The section Latent variables shows the model coefficients. First, under Height =~, are the factor loadings. The column Estimate shows the raw, unstandardised loadings. The unstandardised loading for meters is \\(1.000\\). This was fixed to exactly \\(1\\) to identify the scale of the latent variable. This means that the latent variable is measured in exactly the same units of measurement as is the observed variable y1_metres. The latent variable is therefore measured in meters. This means that a one-unit change in the LV results in a one-unit change in observed height in meters. The unstandardised loading for y2_feet is \\(3.27\\), i.e. a 1 unit change in the LV (which is in meters) results in \\(3.27\\) units change in feet. This is pretty much what we would expect, because a meter is about \\(3.27\\) feet. Ideally it should be \\(3.28\\), but our measurement error must have a mean that is not quite exactly zero. We apply the same interpretation for y3_inches. The unstandardised loading is \\(38.77\\), which is pretty close to the known scaling factor of \\(39.37\\) (i.e. we know that there are \\(39.37\\) inches in a metre). So, this is what the unstandardised loadings tell us: they tell us the scale of measurement of each of the indicators, in relation to the scale of the variable with its loading fixed to one. By fixing one of the loadings to one, we fix the scale of the latent variable to the same scale as that variable. Notice that both of the loadings for y2_feet and y3_inches are underestimates of what we know to be the true scaling factors. This is because of the measurement error. How much have they been underestimated? We can see this in the standardised loadings. The standardized loadings are in the column Std.all. These have been re-scaled to represent units of standard deviations. The standardised loadings are all about the same for all three measures, just over \\(0.98\\). This is because the measurement error that we added was quite small just \\(2\\) cm on average, which is just over \\(1\\%\\) of the average height of \\(1.7\\) m. We can see from these standardised loadings that, despite their very different scales of measurement, these three measures are all about the same when we take into account how variable they are in their own units. When we put them into the same units, i.e. SDs, they are all about the same. How much measurement error do our measures contain? To get this we must look at the \\(R^2\\) values at the end of the output. These are the proportion of variance in the observed variables that is accounted for by the latent variable. The \\(R^2\\) estimates are all \\(0.96-0.97\\), which means that about \\(97\\%-98\\%\\) of the variance in each of the observed variables is accounted for by the latent variable, and only the remaining \\(2\\%-3\\%\\) is measurement error. This is a very good result, showing that these are all very good, reliable, measures of the latent variable. BONUS What happens if we use “feet” as the reference item to scale the LV? height_feet &lt;- &#39; Height =~ y2_feet + y1_metres + y3_inches &#39; fit_height_feet &lt;- cfa(height_feet, data = height_data) summary(fit_height_feet, standardized = TRUE, rsquare = T) ## lavaan 0.6-18 ended normally after 45 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Height =~ ## y2_feet 1.000 0.344 0.983 ## y1_metres 0.305 0.004 84.821 0.000 0.105 0.984 ## y3_inches 11.845 0.144 82.262 0.000 4.070 0.982 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y2_feet 0.004 0.000 9.937 0.000 0.004 0.033 ## .y1_metres 0.000 0.000 9.632 0.000 0.000 0.032 ## .y3_inches 0.616 0.059 10.413 0.000 0.616 0.036 ## Height 0.118 0.008 15.282 0.000 1.000 1.000 ## ## R-Square: ## Estimate ## y2_feet 0.967 ## y1_metres 0.968 ## y3_inches 0.964 The model has the same fit measures and standardised loadings and \\(R^2\\), but the unstandardised loadings and residual variances have been re-scaled to represent measurements in feet. (i.e. a foot is about a third of a metre, and there are about 12 inches to a foot) "],["overview-3.html", "Overview", " Overview Section 4: Model Fit This section is comprised of one practical in which you will assess the fit of a CFA model on the SF-36 “Mental Health” scale. For this practical, you will require the following files: Data: whitehall_NA.csv . Codebook: codebook for whitehall_NA.docx . This file describes the variables in the above data file. The key variables we’ll be using are: nervous, down, calm, sad, happy. We will treat these responses as continuous scores. Information on SF-36: CFA example - mental health sf-36.docx . This shows the hypothesised latent structure of the SF-36 questionnaire, i.e. what latent variables it is supposed to measure. The data you will analyse were collected as part of the “Whitehall II” study, a longitudinal study of 10,000 London-based civil servants. Full details on the study can be found here: https://www.ucl.ac.uk/psychiatry/research/mental-health-older-people/whitehall-ii Ware 2000 claims that the five “mental health” items of the SF36 health screening questionnaire are indicators of a single “mental health” latent factor. The five items are “nervous”, “down”, “peace”, “sad”, “happy”. Respondents answer each question by saying how frequently they feel the target emotion, from 1 = never to 6 = all the time. Learning Outcomes: build and fit a CFA model; interpret both the unstandardised and standardised loadings, intercepts, and residuals; appreciate the differences between model fit indices (CFI, SRMR, RMSEA) and interpret the resulting values; evaluate single latent constructs. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package read_csv() read a delimited file into tibble readr cfa() fit CFA models lavaan summary() generic function for producing result summaries of models base residuals() extract model residuals stats "],["fitting-cfa-models-a-case-study-using-the-sf-36-mental-health-scale.html", "Fitting CFA Models: A Case Study using the SF-36 “Mental Health” scale Task and Questions", " Fitting CFA Models: A Case Study using the SF-36 “Mental Health” scale Use the lavaan tutorial https://www.lavaan.ugent.be/tutorial/cfa.html to help you carry out the tasks below. You will also require tidyverse and psych. Please make sure to install the psych package if you have not done so before. Task and Questions Fit a CFA model assuming a single latent factor that is indicated by items “nervous”, “down”, “peace”, “sad”, “happy” and include intercepts (meanstructure) in the model. Then answer the questions below. Interpret the factor loadings: what do these tell us? Interpret both the unstandardised and standardised loadings. Remember, you ask for the standardised results by adding standardized = TRUE in the summary() function. Interpret the item intercepts: What do these tell us? Interpret the residuals: What do these tell us? Tip: See https://lavaan.ugent.be/tutorial/inspect.html Interpret the model fit indices (CFI, SRMR, RMSEA): what do these tell us? Tip: You ask for the fit indices by adding fit.measures = TRUE in the summary() function. Overall, do you support the claim made by Ware 2000 that these five items plausibly reflect a single, latent factor? "],["answers-2.html", "Answers The Task Question 1 Question 2 Question 3 Question 4 Question 5", " Answers Section 4 Practical: Fitting CFA Models: A Case Study using the SF-36 “Mental Health” scale The Task Fit a CFA model assuming a single latent factor that is indicated by items “nervous”, “down”, “peace”, “sad”, “happy” and include intercepts (meanstructure) in the model. Then answer the questions below. Let’s load the required packages: library(tidyverse) library(lavaan) library(psych) Importing the data (in this case, my data file is located in a folder called data). whitehall &lt;- read_csv(&quot;data/whitehall_NA.csv&quot;) Let’s now specify the model. mh_fac &lt;- &#39;mh =~ nervous + down + peace + sad + happy&#39; The =~ operator is read as “is indicated by” or “is measured by”. So this translates to: create a latent factor called ‘mh’ that is indicated by the five observed variables listed on the right-hand side of the equation. And now we fit the model. fit_mh_fac &lt;- cfa(mh_fac, data = whitehall, meanstructure = TRUE) Let’s view the parameters estimates (standardised and unstandardised) and fit indices summary(fit_mh_fac, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 23 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 15 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 1663.243 ## Degrees of freedom 5 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.875 ## Tucker-Lewis Index (TLI) 0.750 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -53393.072 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 106816.143 ## Bayesian (BIC) 106921.482 ## Sample-size adjusted Bayesian (SABIC) 106873.815 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.200 ## 90 Percent confidence interval - lower 0.192 ## 90 Percent confidence interval - upper 0.208 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.060 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## mh =~ ## nervous 1.000 0.565 0.572 ## down 1.047 0.023 45.831 0.000 0.592 0.710 ## peace -1.242 0.031 -40.296 0.000 -0.702 -0.580 ## sad 1.349 0.028 48.492 0.000 0.762 0.816 ## happy -1.287 0.030 -43.240 0.000 -0.727 -0.645 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .nervous 1.725 0.011 159.096 0.000 1.725 1.748 ## .down 1.433 0.009 156.627 0.000 1.433 1.720 ## .peace 3.752 0.013 282.320 0.000 3.752 3.101 ## .sad 1.872 0.010 182.334 0.000 1.872 2.003 ## .happy 4.265 0.012 344.279 0.000 4.265 3.782 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .nervous 0.655 0.011 57.504 0.000 0.655 0.672 ## .down 0.343 0.007 49.407 0.000 0.343 0.495 ## .peace 0.971 0.017 57.215 0.000 0.971 0.663 ## .sad 0.292 0.008 35.932 0.000 0.292 0.334 ## .happy 0.743 0.014 54.183 0.000 0.743 0.584 ## mh 0.319 0.012 25.919 0.000 1.000 1.000 And now the residuals. residuals(fit_mh_fac) ## $type ## [1] &quot;raw&quot; ## ## $cov ## nervos down peace sad happy ## nervous 0.000 ## down 0.044 0.000 ## peace 0.007 0.097 0.000 ## sad -0.008 0.037 0.046 0.000 ## happy 0.051 0.065 0.300 0.023 0.000 ## ## $mean ## nervous down peace sad happy ## 0 0 0 0 0 Question 1 Interpret both the standardised and unstandardised loadings: what do these tell us? The factor loadings are shown in the section starting mh =~. Two variables (peace, happy) have negative loadings, the rest positive. This means that the latent factor is “measuring” negative mental health, i.e. people with a high factor score will tend to have high scores on nervous, down, sad and low scores on peace, happy, and vice versa. The first loading, for nervous, is fixed at \\(1\\) to identify the scale of measurement of the latent variable. (We can tell that it is a fixed, and not estimated quantity, because it does not have an SE, z-value, p-value, etc.). This means that the latent variable mh is measured on the same measurement scale as the observed variable nervous, i.e. has the same variance/sd. The size of the other loadings are relative to this loading for nervous. Sad has the highest loading, \\(1.349\\). This means that a one-unit difference in the latent variable would be associated with a \\(1.349\\) unit difference in the score on sad. The raw, unstandardised units for the loadings can be difficult to interpret because we are not familiar with interpreting the somewhat arbitrary 6-point measurement scale for these items. Instead, we often interpret the standardised loadings, which are measured in standard deviations and vary between \\(-1\\) and \\(1\\). These units are found in the Std.all column. The standardised loading for sad is \\(0.816\\), which means that for a one standard deviation difference in the latent variable, we would expect to see a \\(0.816\\) standard deviation difference in the score of the observed variable sad. The variable nervous has the weakest loading, perhaps because nervousness does not always reflect poor mental health (e.g. nervous excitement). Question 2 Interpret the item intercepts: What do these tell us? The item intercepts represent the expected value of the observed variable when the latent variable is zero. Because the latent variable is a standard normal variable, with its mean fixed to zero, the intercepts represent the expected value of the observed variable when the latent variable is at its mean. This is getting a little “chicken and egg”, but in effect it just implies that the intercepts are the mean of the observed items. The Estimates column shows the unstandardised intercepts, i.e. in the original 1-6 scale of the observed variables. Average responses to the positive items (peace, happy) were much higher than those to the negative items. People reported feeling positive quite frequently, and sad/negative quite rarely. Question 3 Interpret the residuals: What do these tell us? The residuals are the differences between the observed and predicted values, in this case of the covariances among the observed variables. The residuals were all below \\(0.1\\), except for the covariance between the positive items happy and peace, which was \\(0.3\\). This suggests a single factor is not picking up the full relationship between these items. This suggests that big proportion of the covariance among these two “positive” items is not being adequately captured by just a single latent factor. Question 4 Interpret the model fit indices (CFI, SRMR, RMSEA): what do these tell us? The model fit indices are shown at the top of the output, starting with Model Test User Model. This gives us the chi-square test of model fit and its associated p-value. The chi-square test is a test of the null hypothesis that the model fits the data perfectly. A non-significant p-value (e.g. \\(p &gt; 0.05\\)) would suggest that the model fits the data well. However, the chi-square test is sensitive to sample size, and so is often significant even when the model fits the data well. Our model does not fit the data well by this criterion. Next we’ll look at the Comparative Fit Index (CFI). This is a measure of how well the model fits the data compared to a null model (a model with zero covariances among the observed variables). A CFI of \\(1\\) would suggest perfect fit, and a CFI of \\(0\\) would suggest our data fit no better than random data. The CFI of \\(0.875\\) is fairly poor. Next up is the Root Mean Square Error of Approximation (RMSEA). This is a measure based on the chi-square, but taking into account the complexity of the model (with complexity represented by estimated model parameters). An RMSEA of \\(0\\) would suggest perfect fit, and a RMSEA of below \\(0.05\\) would suggest a “close” fit between model and data. Our RMSEA of \\(0.2\\) is considered poor. Finally we look at the Standardized Root Mean Square Residual (SRMR). This is a measure of the average of the standardised residuals, i.e. the average error in the model-estimated correlations among the observed variables. An SRMR of \\(0\\) would suggest the model can reproduce the observed correlations perfectly, with all zero residuals. An SRMR of below \\(0.08\\) is considered good. Our SRMR is \\(0.06\\) is good, but as we see from the residuals this is misleading - most residuals are small but one is very large, so on average (which is what the SRMR shows), they are small. Question 5 Overall, do you support the claim made by Ware 2000 that these five items plausibly reflect a single, latent factor? Not good enough. We can see that a single latent construct is not enough to capture the distinction between indicators of negative and positive mental health. "],["overview-4.html", "Overview", " Overview Section 5: Building SEMs This section is comprised of one practical in which we will build on the unidimensional CFA model we considered in Section 4 using data collected as part of the “Whitehall II” study. In Section 4, we saw that the assumption of a single latent “mental health” variable for the SF-36 questionnaire “mental health” items was not supported by the data. For this practical we will assess the fit of a two-dimensional CFA model of the same five SF-36 items. For this practical, you will again require the same files as in Section 4: whitehall_NA.csv, codebook for whitehall_NA.docx, and CFA example - mental health sf-36.docx. Learning Outcomes: build and fit one-factor and two-factor models; interpret the fit and coefficients of one-factor and two-factor models; compare a two-factor model with a one-factor model using the Likelihood Ratio Test. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package read_csv() read a delimited file readr cfa() fit CFA models lavaan summary() generic function for producing result summaries of models base lavResiduals() provides model residuals and standardised residuals, including various summaries lavaan anova() LRT test for comparing (nested) lavaan models lavaan (note that this is different to the anova() function from stats) "],["building-multidimensional-confirmatory-factor-analysis-models.html", "Building Multidimensional Confirmatory Factor Analysis Models Task and Questions", " Building Multidimensional Confirmatory Factor Analysis Models Use the lavaan tutorial https://www.lavaan.ugent.be/tutorial/cfa.html to help you carry out the tasks below and answer the questions. Task and Questions Task 1 Fit a two-factor model of the “mental health” items, (“nervous”, “down”, “peace”, “sad”, “happy”). One factor is to be indicated by the positive items (“happy”, “peace”), and one factor indicated by the negative items (“nervous”, “down”, “sad”). What results suggest that this model is a good model? What results suggest that this model is a poor model? Is this model preferable to a unidimensional model based upon a Likelihood Ratio Test? Task 2 Fit a one-factor model of the “mental health” items, (“nervous”, “down”, “peace”, “sad”, “happy”), just like you did in the previous practical. However, in this model, allow the residual variances for the items happy and peace to correlate with each other. Task 3 The two-dimensional model we’ve looked at earlier assumes that each item contains information about just one latent factor - i.e. each item has only one factor loading. This is unlikely to be true in many circumstances especially when the items relate to nested concepts, e.g. the overarching concept of “mood” encapsulates the more specific concepts “positive mood” and “negative mood”. You will re-cast our two-dimensional mood model as a bifactor model, which allows for this. See if you can fit a model with the following specifications: A general factor “mood” indicated by all five items; A specific factor “positive mood” indicated by the items “happy” and “peace”; The correlation between the general and specific factors is fixed to zero. This model represents the correlation between the latent factors by allowing the two positive mood items to have two factor loading each, one on the general factor and one on the specific factor. How do you interpret the fit and coefficients of this model? "],["answers-3.html", "Answers Task 1 Task 2 Task 3", " Answers Section 5 Practical: Building Multidimensional Confirmatory Factor Analysis Models Let’s load the required packages: library(lavaan) library(tidyverse) And import the data. whitehall &lt;- read_csv(&quot;data/whitehall_NA.csv&quot;) Task 1 Fit a two-factor model of the “mental health” items, (“nervous”, “down”, “peace”, “sad”, “happy”). One factor is to be indicated by the positive items (“happy”, “peace”), and one factor indicated by the negative items (“nervous”, “down”, “sad”). Let’s construct the two-factor model whereby: neg =~ sad + nervous + down is the negative mood pos =~ happy + peace is the positive mood pos ~~ neg is the factor correlation mh_fac2 &lt;- &#39;neg =~ sad + nervous + down pos =~ happy + peace pos ~~ neg &#39; Now let’s fit the model to the obtain and have a look at the results. fit_mh_fac2 &lt;- cfa(mh_fac2, data = whitehall, meanstructure = TRUE) summary(fit_mh_fac2, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 200.351 ## Degrees of freedom 4 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.963 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -52661.626 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 105355.252 ## Bayesian (BIC) 105467.613 ## Sample-size adjusted Bayesian (SABIC) 105416.768 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.077 ## 90 Percent confidence interval - lower 0.068 ## 90 Percent confidence interval - upper 0.086 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.303 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## neg =~ ## sad 1.000 0.792 0.847 ## nervous 0.713 0.015 47.897 0.000 0.564 0.571 ## down 0.775 0.013 59.175 0.000 0.614 0.737 ## pos =~ ## happy 1.000 0.936 0.829 ## peace 0.926 0.019 47.694 0.000 0.867 0.716 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## neg ~~ ## pos -0.511 0.013 -40.378 0.000 -0.690 -0.690 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 1.872 0.010 182.334 0.000 1.872 2.003 ## .nervous 1.725 0.011 159.095 0.000 1.725 1.748 ## .down 1.433 0.009 156.627 0.000 1.433 1.720 ## .happy 4.265 0.012 344.279 0.000 4.265 3.782 ## .peace 3.752 0.013 282.320 0.000 3.752 3.101 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 0.247 0.009 27.504 0.000 0.247 0.282 ## .nervous 0.657 0.011 57.380 0.000 0.657 0.673 ## .down 0.317 0.007 44.885 0.000 0.317 0.457 ## .happy 0.397 0.017 23.512 0.000 0.397 0.312 ## .peace 0.713 0.017 40.846 0.000 0.713 0.487 ## neg 0.627 0.015 40.875 0.000 1.000 1.000 ## pos 0.875 0.024 35.749 0.000 1.000 1.000 To obtain further information, we can use the LavResiduals() function. lavResiduals(fit_mh_fac2) ## $type ## [1] &quot;cor.bentler&quot; ## ## $cov ## sad nervos down happy peace ## sad 0.000 ## nervous -0.026 0.000 ## down 0.002 0.040 0.000 ## happy -0.019 0.004 0.033 0.000 ## peace -0.014 -0.043 0.048 0.000 0.000 ## ## $mean ## sad nervous down happy peace ## 0 0 0 0 0 ## ## $cov.z ## sad nervos down happy peace ## sad 0.000 ## nervous -12.370 0.000 ## down 2.381 9.235 0.000 ## happy -8.171 0.579 8.564 0.000 ## peace -5.085 -6.139 10.900 0.000 0.000 ## ## $mean.z ## sad nervous down happy peace ## 0 0 0 0 0 ## ## $summary ## cov mean total ## srmr 0.023 0.00 0.020 ## srmr.se 0.001 NA 0.001 ## srmr.exactfit.z 15.720 NA 15.720 ## srmr.exactfit.pvalue 0.000 NA 0.000 ## usrmr 0.023 0.00 0.020 ## usrmr.se 0.002 NA 0.001 ## usrmr.ci.lower 0.020 NA 0.018 ## usrmr.ci.upper 0.026 NA 0.022 ## usrmr.closefit.h0.value 0.050 0.05 0.050 ## usrmr.closefit.z -16.347 NA -21.052 ## usrmr.closefit.pvalue 1.000 NA 1.000 Question 1: What makes this a good model? The CFI of 0.985 indicates close fit. The SRMR of \\(0.02\\) indicates close fit. The std.all loadings for all but one items are good (\\(&gt;0.7\\)). The correlation between the factors (\\(-0.69\\)) is negative, as expected, and not so large as to suggest a single factor Question 2: What makes this a bad model? The RMSEA of \\(0.077\\) is above \\(0.05\\), indicating some misfit. The model chi-square of \\(200.351\\) is much lower than for the unidimensional model, but is still significant. The std.all loading for one item (nervous) is weak (\\(0.57\\)). There are still some non-trivial residuals for some items (e.g. peace) Question 3: Compare the two-factor model with a one-factor model using LRT First, we fit the one-factor model. mh_fac &lt;- &#39;mh =~ nervous + down + peace + sad + happy&#39; fit_mh_fac &lt;- cfa(mh_fac, data = whitehall, meanstructure = TRUE) We then run the Likelihood Ratio Test of one- vs. two-factor models. anova(fit_mh_fac, fit_mh_fac2) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## fit_mh_fac2 4 105355 105468 200.35 ## fit_mh_fac 5 106816 106921 1663.24 1462.9 0.41998 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The LR Chi-square is \\(1462.9\\), \\(df = 1\\), \\(p &lt; 0.001\\). There is a strong significant difference in model fit (i.e. chi-square difference). The two-factor model is strongly preferred to the one-factor model Task 2 Fit a one-factor model of the “mental health” items, (“nervous”, “down”, “peace”, “sad”, “happy”), just like you did in the previous practical. However, in this model, allow the residual variances for the items happy and peace to correlate with each other. Let’s fit the model and explore the results. mh_fac_cor &lt;- &#39;mh =~ nervous + down + peace + sad + happy # allow happy and peace to have a residual correlation happy ~~ peace &#39; fit_mh_fac_cor &lt;- cfa(mh_fac_cor, data = whitehall, meanstructure = TRUE) summary(fit_mh_fac_cor, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 200.351 ## Degrees of freedom 4 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.963 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -52661.626 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 105355.252 ## Bayesian (BIC) 105467.613 ## Sample-size adjusted Bayesian (SABIC) 105416.768 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.077 ## 90 Percent confidence interval - lower 0.068 ## 90 Percent confidence interval - upper 0.086 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.303 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## mh =~ ## nervous 1.000 0.564 0.571 ## down 1.088 0.023 46.722 0.000 0.614 0.737 ## peace -1.060 0.030 -35.724 0.000 -0.598 -0.494 ## sad 1.403 0.029 47.897 0.000 0.792 0.847 ## happy -1.144 0.029 -39.895 0.000 -0.646 -0.572 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .peace ~~ ## .happy 0.425 0.013 32.315 0.000 0.425 0.437 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .nervous 1.725 0.011 159.095 0.000 1.725 1.748 ## .down 1.433 0.009 156.627 0.000 1.433 1.720 ## .peace 3.752 0.013 282.320 0.000 3.752 3.101 ## .sad 1.872 0.010 182.334 0.000 1.872 2.003 ## .happy 4.265 0.012 344.279 0.000 4.265 3.782 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .nervous 0.657 0.011 57.380 0.000 0.657 0.673 ## .down 0.317 0.007 44.885 0.000 0.317 0.457 ## .peace 1.106 0.019 59.530 0.000 1.106 0.756 ## .sad 0.247 0.009 27.504 0.000 0.247 0.282 ## .happy 0.855 0.015 57.181 0.000 0.855 0.672 ## mh 0.318 0.012 25.800 0.000 1.000 1.000 The model fit indices are identical to the two-dimensional model! Allowing the residual variances of the positive items to correlate is doing the same thing that the two-factor model did - it’s capturing the relationship between the positive items that the single factor could not capture on its own. In the two-factor model, this relationship was captured by the factor, here it’s captured by the residual correlation. These are two different ways of modelling the same thing. These are equivalent models, that have the same number of model parameters and the same model fit. Task 3 The bifactor model - fit a model with the following specifications: A general factor “mood” indicated by all five items; A specific factor “positive mood” indicated by the items “happy” and “peace”; The correlation between the general and specific factors is fixed to zero. Specifying the model whereby: general =~ sad + nervous + down + happy + peace is the general factor pos =~ happy + peace is the specific positive factor general ~~ 0*pos is factor correlation = zero mh_bifac &lt;- &#39;general =~ sad + nervous + down + happy + peace pos =~ happy + peace general ~~ 0*pos &#39; Fitting the model to the data and exploring the results. fit_mh_bifac &lt;- cfa(mh_bifac, data = whitehall, meanstructure = TRUE) summary(fit_mh_bifac, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 17 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 200.351 ## Degrees of freedom 3 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.950 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -52661.626 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 105357.252 ## Bayesian (BIC) 105476.635 ## Sample-size adjusted Bayesian (SABIC) 105422.612 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.089 ## 90 Percent confidence interval - lower 0.079 ## 90 Percent confidence interval - upper 0.100 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.929 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## general =~ ## sad 1.000 0.792 0.847 ## nervous 0.713 NA 0.564 0.571 ## down 0.775 NA 0.614 0.737 ## happy -0.815 NA -0.646 -0.572 ## peace -0.755 NA -0.598 -0.494 ## pos =~ ## happy 1.000 0.730 0.648 ## peace 0.796 NA 0.581 0.481 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## general ~~ ## pos 0.000 0.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 1.872 NA 1.872 2.003 ## .nervous 1.725 NA 1.725 1.748 ## .down 1.433 NA 1.433 1.720 ## .happy 4.265 NA 4.265 3.782 ## .peace 3.752 NA 3.752 3.101 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 0.247 NA 0.247 0.282 ## .nervous 0.657 NA 0.657 0.673 ## .down 0.317 NA 0.317 0.457 ## .happy 0.322 NA 0.322 0.253 ## .peace 0.768 NA 0.768 0.525 ## general 0.627 NA 1.000 1.000 ## pos 0.534 NA 1.000 1.000 Question: How do you interpret the fit and coefficients of this model? The results look odd - there are no SEs for many parameters. The model looks identified, and the parameter estimates look sensible, so what might be going on? Perhaps the assumptions of the SEs have been violated. We can estimate the SEs using weaker assumptions - the bootstrap method. This is computationally intensive, so it may take a minute to estimate. fit_mh_bifac &lt;- cfa(mh_bifac, data = whitehall, meanstructure = TRUE, se = &quot;bootstrap&quot;) summary(fit_mh_bifac, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 17 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 200.351 ## Degrees of freedom 3 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.950 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -52661.626 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 105357.252 ## Bayesian (BIC) 105476.635 ## Sample-size adjusted Bayesian (SABIC) 105422.612 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.089 ## 90 Percent confidence interval - lower 0.079 ## 90 Percent confidence interval - upper 0.100 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.929 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## general =~ ## sad 1.000 0.792 0.847 ## nervous 0.713 0.022 32.350 0.000 0.564 0.571 ## down 0.775 0.015 50.107 0.000 0.614 0.737 ## happy -0.815 0.023 -34.992 0.000 -0.646 -0.572 ## peace -0.755 0.025 -30.534 0.000 -0.598 -0.494 ## pos =~ ## happy 1.000 0.730 0.648 ## peace 0.796 0.012 67.921 0.000 0.581 0.481 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## general ~~ ## pos 0.000 0.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 1.872 0.011 175.206 0.000 1.872 2.003 ## .nervous 1.725 0.011 155.451 0.000 1.725 1.748 ## .down 1.433 0.009 156.164 0.000 1.433 1.720 ## .happy 4.265 0.013 339.181 0.000 4.265 3.782 ## .peace 3.752 0.014 271.012 0.000 3.752 3.101 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 0.247 0.011 21.462 0.000 0.247 0.282 ## .nervous 0.657 0.018 37.030 0.000 0.657 0.673 ## .down 0.317 0.014 22.967 0.000 0.317 0.457 ## .happy 0.322 0.012 25.798 0.000 0.322 0.253 ## .peace 0.768 0.016 47.869 0.000 0.768 0.525 ## general 0.627 0.022 29.044 0.000 1.000 1.000 ## pos 0.534 0.017 31.991 0.000 1.000 1.000 Let’s now compare it with the earlier two-factor model. summary(fit_mh_fac2, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-18 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Used Total ## Number of observations 8288 10308 ## ## Model Test User Model: ## ## Test statistic 200.351 ## Degrees of freedom 4 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 13261.213 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.985 ## Tucker-Lewis Index (TLI) 0.963 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -52661.626 ## Loglikelihood unrestricted model (H1) -52561.450 ## ## Akaike (AIC) 105355.252 ## Bayesian (BIC) 105467.613 ## Sample-size adjusted Bayesian (SABIC) 105416.768 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.077 ## 90 Percent confidence interval - lower 0.068 ## 90 Percent confidence interval - upper 0.086 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.303 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## neg =~ ## sad 1.000 0.792 0.847 ## nervous 0.713 0.015 47.897 0.000 0.564 0.571 ## down 0.775 0.013 59.175 0.000 0.614 0.737 ## pos =~ ## happy 1.000 0.936 0.829 ## peace 0.926 0.019 47.694 0.000 0.867 0.716 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## neg ~~ ## pos -0.511 0.013 -40.378 0.000 -0.690 -0.690 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 1.872 0.010 182.334 0.000 1.872 2.003 ## .nervous 1.725 0.011 159.095 0.000 1.725 1.748 ## .down 1.433 0.009 156.627 0.000 1.433 1.720 ## .happy 4.265 0.012 344.279 0.000 4.265 3.782 ## .peace 3.752 0.013 282.320 0.000 3.752 3.101 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .sad 0.247 0.009 27.504 0.000 0.247 0.282 ## .nervous 0.657 0.011 57.380 0.000 0.657 0.673 ## .down 0.317 0.007 44.885 0.000 0.317 0.457 ## .happy 0.397 0.017 23.512 0.000 0.397 0.312 ## .peace 0.713 0.017 40.846 0.000 0.713 0.487 ## neg 0.627 0.015 40.875 0.000 1.000 1.000 ## pos 0.875 0.024 35.749 0.000 1.000 1.000 The chi-squre model fit is THE SAME (\\(200.351\\)), but because the bifactor model is slightly more complex (it has one extra parameter, so one fewer df), the RMSEA is a bit worse. In terms of the factor loadings, the standardized factor loadings for the general factor are good ($ &gt; 0.7\\() for sad and down, but fairly poor (\\)&lt;0.6$) for the others. This suggests that these items do not really tap into a general mood factor very well. The standardized loadings for the pos factor are also weak, so the specific factor is not really providing a good explanation for the positive items, either after accounting for the general factor. Considering these bifactor results alongside the separate factors model, these are two different conceptual models. Both have similar fit, but the coefficients seem to support the two-factor model better, and the extra complexith of the bifactor model isn’t repaid by better model fit. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
